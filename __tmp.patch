diff --git a/AGI_Evolutive/core/cognitive_architecture.py b/AGI_Evolutive/core/cognitive_architecture.py index c38fae1d0a033007070902b2ee036ad1ff3b2343..26526e20636d6f5ce47b7057149cba1e7e476302 100644 --- a/AGI_Evolutive/core/cognitive_architecture.py +++ b/AGI_Evolutive/core/cognitive_architecture.py @@ -48,59 +48,70 @@ from AGI_Evolutive.runtime.response import ( ensure_contract, format_agent_reply, humanize_reasoning_block, ) from AGI_Evolutive.runtime.scheduler import Scheduler from AGI_Evolutive.runtime.job_manager import JobManager from AGI_Evolutive.world_model import PhysicsEngine from AGI_Evolutive.self_improver import SelfImprover from AGI_Evolutive.self_improver.code_evolver import CodeEvolver from AGI_Evolutive.self_improver.promote import PromotionManager from AGI_Evolutive.self_improver.skill_acquisition import SkillSandboxManager from AGI_Evolutive.planning.htn import HTNPlanner from AGI_Evolutive.core.persistence import PersistenceManager from AGI_Evolutive.core.self_model import SelfModel from AGI_Evolutive.core.config import cfg from AGI_Evolutive.utils.jsonsafe import json_sanitize from AGI_Evolutive.utils.llm_service import try_call_llm_dict logger = logging.getLogger(__name__) class CognitiveArchitecture: """Central coordinator for the agent's cognitive subsystems.""" - def __init__(self, boot_minimal: bool = False): + def __init__( + self, + boot_minimal: bool = False, + *, + enable_boot_telemetry: bool = False, + ): self.boot_minimal = boot_minimal + self._boot_telemetry_enabled = bool(enable_boot_telemetry) logger.info( "Initialisation de la CognitiveArchitecture", extra={"boot_minimal": bool(boot_minimal)}, ) # Observability self.logger = JSONLLogger("runtime/agent_events.jsonl") self.telemetry = Telemetry() + if self._boot_telemetry_enabled: + try: + self.telemetry.enable_console(True) + except Exception: + pass self.style_policy = StylePolicy() self.intent_model = IntentModel() self.question_manager = QuestionManager(self) self._last_intent_decay = time.time() self.goal_dag = GoalDAG("runtime/goal_dag.json") # Global state self.global_activation = 0.5 self.start_time = time.time() self.reflective_mode = True self.last_output_text = "OK" self.last_user_id = "default" self._memory_request_goal_id: Optional[str] = None self.rag_adaptive: Optional[RAGAdaptiveController] = None self._rag_last_context: Optional[Dict[str, Any]] = None self._rag_feedback_queue = deque(maxlen=24) # Core subsystems self.telemetry.log("init", "core", {"stage": "memory"}) self.vector_store = VectorStore() self.memory = MemorySystem(self) from AGI_Evolutive.memory.semantic_manager import ( # type: ignore # local import avoids circular init SemanticMemoryManager, ) @@ -297,51 +308,54 @@ class CognitiveArchitecture: if "style.hedging" in ov: style_policy.params["hedging"] = max(0.0, min(1.0, float(ov["style.hedging"]))) if "learning.self_assess.threshold" in ov and hasattr(arch, "learning"): try: threshold = float(ov["learning.self_assess.threshold"]) setattr(arch.learning, "self_assess_threshold", threshold) except Exception: pass abduction = getattr(arch, "abduction", None) if not abduction: return if "abduction.tie_gap" in ov: setattr(abduction, "tie_gap", float(ov["abduction.tie_gap"])) if "abduction.weights.prior" in ov: setattr(abduction, "w_prior", float(ov["abduction.weights.prior"])) if "abduction.weights.boost" in ov: setattr(abduction, "w_boost", float(ov["abduction.weights.boost"])) if "abduction.weights.match" in ov: setattr(abduction, "w_match", float(ov["abduction.weights.match"])) def _arch_factory(overrides: Dict[str, Any]) -> "CognitiveArchitecture": fresh: "CognitiveArchitecture" try: - fresh = self.__class__(boot_minimal=True) + fresh = self.__class__( + boot_minimal=True, + enable_boot_telemetry=self._boot_telemetry_enabled, + ) except Exception: fresh = self try: _apply_overrides(fresh, overrides or {}) except Exception: pass return fresh self._arch_factory = _arch_factory self.promotions: Optional[PromotionManager] if not boot_minimal: self.self_improver = SelfImprover( arch_factory=self._arch_factory, memory=self.memory, question_manager=getattr(self, "question_manager", None), apply_overrides=lambda overrides: _apply_overrides(self, overrides), ) self.promotions = self.self_improver.prom self.code_evolver: Optional[CodeEvolver] = getattr(self.self_improver, "code_evolver", None) else: self.self_improver = None self.promotions = None self.code_evolver = None # Persistence layer shared with Autopilot/logger diff --git a/AGI_Evolutive/main.py b/AGI_Evolutive/main.py index 0e57a0127c35b938762f54680f2f4f8a2f505559..11d494fa9258a34f2d4b32d5cf76029413790f88 100644 --- a/AGI_Evolutive/main.py +++ b/AGI_Evolutive/main.py @@ -1,26 +1,27 @@ # 🚀 main.py - Point d'entrée AGI Évolutive +import argparse import glob import json import logging import os import re import sys import time import traceback import unicodedata from datetime import datetime from typing import Any, Dict, List, Optional, Tuple from AGI_Evolutive.utils.llm_service import try_call_llm_dict logger = logging.getLogger(__name__) try: from AGI_Evolutive.language.quote_memory import QuoteMemory # type: ignore except ImportError: # pragma: no cover - module optionnel QuoteMemory = None # type: ignore try: from AGI_Evolutive.social.tactic_selector import TacticSelector # type: ignore except ImportError: # pragma: no cover - module optionnel TacticSelector = None # type: ignore @@ -289,77 +290,111 @@ class CLIAdaptiveFeedback: self._record_event( label=sentiment, success=None, confidence=1.0, origin="llm", heuristic=str(urgency) if isinstance(urgency, str) else None, ) return response def record_prediction(self, label: str, confidence: float) -> None: self._record_event( label=label, success=None, confidence=confidence, origin="classifier", heuristic="classifier", ) def list_inbox(inbox_dir="inbox"): files = [os.path.basename(p) for p in glob.glob(os.path.join(inbox_dir, "*"))] if not files: print("📂 Inbox vide.") else: print("📁 Inbox :", ", ".join(files)) -def run_cli(): +def _build_arg_parser() -> argparse.ArgumentParser: + parser = argparse.ArgumentParser( + description="Interface en ligne de commande pour AGI Évolutive", + ) + parser.add_argument( + "--boot-minimal", + action="store_true", + help="Initialise l'architecture en mode minimal pour accélérer le démarrage.", + ) + parser.add_argument( + "--no-auto-llm", + action="store_true", + help="Désactive l'activation automatique du LLM au démarrage.", + ) + parser.add_argument( + "--boot-progress", + action="store_true", + help="Affiche les étapes de chargement de l'architecture cognitive.", + ) + return parser + + +def run_cli(argv: Optional[List[str]] = None): log_path = configure_logging() logger = logging.getLogger(__name__) + parser = _build_arg_parser() + args = parser.parse_args(argv) + print(BANNER) print(f"📝 Journaux: {log_path}") print("Chargement de l'architecture cognitive…") + if args.boot_minimal: + print("⚡ Mode minimal activé : certaines capacités avancées sont désactivées pour un démarrage rapide.") + print( + "⏳ Initialisation en cours. L'invite > apparaîtra une fois la phrase " + "✅ AGI initialisée affichée. Cela peut prendre plusieurs minutes lors du premier lancement." + ) llm_auto_enabled = False - if not os.getenv("AGI_DISABLE_LLM"): + if not args.no_auto_llm and not os.getenv("AGI_DISABLE_LLM"): try: manager = get_llm_manager() if not manager.enabled: manager.set_enabled(True) llm_auto_enabled = True except Exception as exc: # pragma: no cover - uniquement log utilisateur logger.warning( "Impossible d'activer automatiquement le LLM : %s", exc, exc_info=True ) if llm_auto_enabled: logger.info("LLM activé automatiquement au démarrage de l'IA") print("🤖 Intégration LLM activée automatiquement.") logger.info("Démarrage de la CLI AGI Évolutive", extra={"log_path": str(log_path)}) logger.info("Initialisation de l'architecture cognitive") try: - arch = CognitiveArchitecture() + arch = CognitiveArchitecture( + boot_minimal=args.boot_minimal, + enable_boot_telemetry=args.boot_progress, + ) arch.prioritizer = getattr(arch, "prioritizer", GoalPrioritizer(arch)) from AGI_Evolutive.cognition.trigger_bus import TriggerBus # déjà importable from AGI_Evolutive.cognition.evolution_manager import EvolutionManager try: bus = getattr(arch.prioritizer, "trigger_bus", None) if bus and hasattr(bus, "set_habit_strength_source"): bus.set_habit_strength_source(EvolutionManager.shared().habits_strength) except Exception: pass # --- bootstrap voix & contexte --- arch.voice_profile = getattr( arch, "voice_profile", VoiceProfile(arch.self_model, user_model=getattr(arch, "user_model", None)), ) arch.lexicon = getattr(arch, "lexicon", LiveLexicon()) arch.style_observer = getattr( arch, "style_observer", StyleObserver( arch.self_model, getattr(arch, "homeostasis", None), arch.voice_profile, arch.lexicon, diff --git a/docs/cli_troubleshooting.md b/docs/cli_troubleshooting.md new file mode 100644 index 0000000000000000000000000000000000000000..e662652ad89a26346eb465c325bcf6d476aa9ef8 --- /dev/null +++ b/docs/cli_troubleshooting.md @@ -0,0 +1,47 @@ +# AGI Évolutive CLI – Démarrage & Dépannage + +La CLI d'AGI Évolutive charge une architecture cognitive très complète. Sur +certaines machines, surtout lors du premier lancement, la séquence +d'initialisation peut durer plusieurs dizaines de secondes pendant +l'importation des modèles et la mise en place des sous-systèmes. Pendant cette +phase, aucune invite n'est affichée et les commandes comme /quit ne sont pas +encore disponibles : il faut attendre l'apparition du message ✅ AGI initialisée +ainsi que du prompt > . + +## Options utiles + +La commande python -m AGI_Evolutive.main accepte désormais plusieurs options +pour adapter le démarrage : + +* --boot-minimal : démarre l'architecture en mode réduit pour obtenir une + invite rapidement. Certaines fonctionnalités avancées (RAG, auto-amélioration, + etc.) restent désactivées tant que vous n'avez pas relancé sans ce drapeau. +* --boot-progress : affiche les étapes internes du chargement (mémoire, + perception, métacognition…). Pratique pour vérifier que l'initialisation + progresse toujours et repérer un éventuel blocage. +* --no-auto-llm : empêche l'activation automatique de l'intégration LLM si + vous souhaitez tout contrôler manuellement ou travailler hors connexion. + +Exemples : + + bash +# Démarrage rapide pour un test +python -m AGI_Evolutive.main --boot-minimal --boot-progress + +# Session complète mais sans activer automatiquement le LLM +python -m AGI_Evolutive.main --no-auto-llm + + +## Astuces supplémentaires + +* Le premier lancement peut être plus long, car certains modèles (ex. vecteurs + sémantiques) sont générés ou téléchargés. Les exécutions suivantes seront + nettement plus rapides. +* Si la fenêtre console semble figée, vérifiez qu'aucune sélection de texte n'est + active (mode « QuickEdit » sur Windows). Dans ce mode, l'entrée clavier est + temporairement suspendue. +* Ctrl+C force une interruption propre. L'agent déclenche alors une sauvegarde + et se ferme. + +Ces options offrent un retour visuel immédiat et permettent d'interagir avec la +CLI même sur un matériel plus limité. EOF )
