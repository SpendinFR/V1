diff a/AIVIKI-main/AGI_Evolutive/orchestrator.py b/AIVIKI-main/AGI_Evolutive/orchestrator.py	(rejected hunks)
@@ -42,50 +42,55 @@ from AGI_Evolutive.cognition.pipelines_registry import (
 )
 from AGI_Evolutive.core.config import load_config
 from AGI_Evolutive.core.decision_journal import DecisionJournal
 from AGI_Evolutive.core.evaluation import get_last_priority_token, unified_priority
 from AGI_Evolutive.core.reasoning_ledger import ReasoningLedger
 from AGI_Evolutive.core.policy import PolicyEngine
 from AGI_Evolutive.core.selfhood_engine import SelfhoodEngine
 from AGI_Evolutive.core.self_model import SelfModel
 from AGI_Evolutive.core.telemetry import Telemetry
 from AGI_Evolutive.core.timeline_manager import TimelineManager
 from AGI_Evolutive.core.trigger_types import Trigger, TriggerType
 from AGI_Evolutive.emotions.emotion_engine import EmotionEngine
 from AGI_Evolutive.goals.curiosity import CuriosityEngine
 from AGI_Evolutive.io.action_interface import ActionInterface
 from AGI_Evolutive.io.intent_classifier import classify
 from AGI_Evolutive.io.perception_interface import PerceptionInterface
 from AGI_Evolutive.memory.concept_extractor import ConceptExtractor
 from AGI_Evolutive.memory.concept_store import ConceptStore
 from AGI_Evolutive.memory.consolidator import Consolidator
 from AGI_Evolutive.memory.episodic_linker import EpisodicLinker
 from AGI_Evolutive.memory.memory_store import MemoryStore
 from AGI_Evolutive.memory.semantic_bridge import SemanticMemoryBridge
 from AGI_Evolutive.light_scheduler import LightScheduler
 from AGI_Evolutive.runtime.job_manager import JobManager
 from AGI_Evolutive.runtime.phenomenal_kernel import ModeManager, PhenomenalKernel
+from AGI_Evolutive.phenomenology import (
+    PhenomenalJournal,
+    PhenomenalQuestioner,
+    PhenomenalRecall,
+)
 from AGI_Evolutive.runtime.system_monitor import SystemMonitor
 from AGI_Evolutive.utils.llm_service import (
     LLMIntegrationError,
     LLMUnavailableError,
     get_llm_manager,
     is_llm_enabled,
     try_call_llm_dict,
 )
 
 
 logger = logging.getLogger(__name__)
 
 
 def _llm_enabled() -> bool:
     return is_llm_enabled()
 
 
 def _llm_manager():
     return get_llm_manager()
 
 
 # --- seuils / cadence (tunable) ---
 _DEFAULT_SJ_CONF = {
     "surprise_threshold": 0.50,
     "contradiction_boost": 0.10,
@@ -1082,72 +1087,99 @@ class Orchestrator:
         )
         existing_interface = getattr(self.arch, "action_interface", None)
         bound: Dict[str, Any] = {}
         existing_jobs = None
         if isinstance(existing_interface, ActionInterface):
             bound = getattr(existing_interface, "bound", {}) or {}
             existing_jobs = bound.get("jobs")
 
         if existing_jobs is None:
             arch_jobs = getattr(self.arch, "jobs", None)
             if isinstance(arch_jobs, JobManager):
                 existing_jobs = arch_jobs
 
         if isinstance(existing_interface, ActionInterface):
             self._action_interface = existing_interface
         else:
             self._action_interface = ActionInterface(self._memory_store)
 
         if existing_jobs is None:
             job_manager = JobManager(self)
         else:
             job_manager = existing_jobs
 
         self.job_manager = job_manager
         self._phenomenal_kernel = PhenomenalKernel()
+        self.phenomenal_journal = getattr(self, "phenomenal_journal", None) or PhenomenalJournal()
+        self.phenomenal_recall = getattr(self, "phenomenal_recall", None) or PhenomenalRecall(
+            self.phenomenal_journal
+        )
+        self.phenomenal_questioner = getattr(self, "phenomenal_questioner", None) or PhenomenalQuestioner(
+            self.phenomenal_journal
+        )
         self._mode_manager = ModeManager(
             target_work_ratio=0.8,
             window=900.0,
             enter_energy_threshold=0.38,
             exit_energy_threshold=0.55,
             exit_veto_threshold=0.45,
             min_switch_interval=75.0,
         )
         self._system_monitor = SystemMonitor(interval=3.0)
         self._last_system_snapshot: Dict[str, Any] = {}
         self._pending_system_alerts: List[Dict[str, Any]] = []
         self._last_system_slowdown: float = 0.0
         self._last_system_slowdown_ts: float = 0.0
         self.phenomenal_kernel_state: Dict[str, Any] = {}
         self.current_mode: str = "travail"
         self._last_intrinsic_reward_ts: float = 0.0
         self._intrinsic_reward_cooldown = 90.0
         self._hedonic_reward_min_gain = 0.4
         try:
             setattr(self.arch, "phenomenal_kernel_state", self.phenomenal_kernel_state)
         except Exception:
             pass
+        try:
+            setattr(self.arch, "phenomenal_journal", self.phenomenal_journal)
+            setattr(self.arch, "phenomenal_recall", self.phenomenal_recall)
+            arch_memory = getattr(self.arch, "memory", None)
+            if arch_memory is not None:
+                if hasattr(arch_memory, "set_phenomenal_sources"):
+                    arch_memory.set_phenomenal_sources(
+                        journal=self.phenomenal_journal,
+                        recall=self.phenomenal_recall,
+                    )
+                else:
+                    setattr(arch_memory, "phenomenal_journal", self.phenomenal_journal)
+                    setattr(arch_memory, "phenomenal_recall", self.phenomenal_recall)
+        except Exception:
+            pass
+        try:
+            setattr(self._meta, "phenomenal_journal", self.phenomenal_journal)
+            setattr(self._meta, "phenomenal_recall", self.phenomenal_recall)
+        except Exception:
+            pass
         self._job_base_budgets = {
             queue: info.get("max_running", 1)
             for queue, info in self.job_manager.budgets.items()
         }
         self._need_directives: List[Dict[str, Any]] = []
         self._need_directives_lock = threading.RLock()
         self._last_llm_recommendations: Optional[Mapping[str, Any]] = None
 
         if isinstance(existing_interface, ActionInterface):
             bind_kwargs = {}
             if bound.get("arch") is None:
                 bind_kwargs["arch"] = self.arch
             if bound.get("goals") is None:
                 bind_kwargs["goals"] = getattr(self.arch, "goals", None)
             if bound.get("policy") is None:
                 bind_kwargs["policy"] = getattr(self.arch, "policy", None)
             if bound.get("metacog") is None:
                 bind_kwargs["metacog"] = getattr(self.arch, "metacognition", None)
             if bound.get("emotions") is None:
                 bind_kwargs["emotions"] = getattr(self.arch, "emotions", None)
             if bound.get("language") is None:
                 bind_kwargs["language"] = getattr(self.arch, "language", None)
             if bound.get("simulator") is None:
                 bind_kwargs["simulator"] = getattr(self.arch, "simulator", None)
             if bound.get("memory") is None and self._memory_store is not None:
@@ -1175,50 +1207,52 @@ class Orchestrator:
             )
         self._perception_interface = PerceptionInterface(self._memory_store)
         self.curiosity = CuriosityEngine(architecture=self.arch)
 
         self.scheduler = LightScheduler()
         self._register_jobs()
 
         self.trigger_bus = TriggerBus()
         self.trigger_router = TriggerRouter()
         self.immediate_question_blocked = False
 
         self.memory = SimpleNamespace(
             store=_MemoryStoreAdapter(self._memory_store),
             consolidator=_ConsolidatorAdapter(self._consolidator, self._memory_store),
             concepts=_ConceptAdapter(
                 self._concepts,
                 self._memory_store,
                 lock=self._semantic_lock,
             ),
             episodic=_EpisodicAdapter(
                 self._episodic,
                 self._memory_store,
                 lock=self._semantic_lock,
             ),
         )
+        setattr(self.memory, "phenomenal_journal", self.phenomenal_journal)
+        setattr(self.memory, "phenomenal_recall", self.phenomenal_recall)
 
         self.thinking_monitor = getattr(self, "thinking_monitor", None) or ThinkingMonitor()
         self.understanding_agg = getattr(self, "understanding_agg", None) or UnderstandingAggregator()
         self.selfhood = getattr(self, "selfhood", None) or SelfhoodEngine()
         self.reasoning_ledger = getattr(self, "reasoning_ledger", None) or ReasoningLedger(
             memory_store=self.memory.store if hasattr(self.memory, "store") else None
         )
         self.decision_journal = getattr(self, "decision_journal", None) or DecisionJournal(
             memory_store=self.memory.store if hasattr(self.memory, "store") else None
         )
         self.timeline = getattr(self, "timeline", None) or TimelineManager(
             memory_store=self.memory.store if hasattr(self.memory, "store") else None
         )
         self.io = SimpleNamespace(
             perception=_PerceptionAdapter(self._perception_interface),
             action=_ActionAdapter(self._action_interface),
         )
         try:
             self._emotion_engine.bind(
                 arch=self,
                 memory=getattr(self.arch, "memory", None),
                 metacog=self._meta,
                 goals=getattr(self.arch, "goals", None),
                 language=getattr(self.arch, "language", None),
                 evolution=self._evolution,
@@ -2231,101 +2265,144 @@ class Orchestrator:
         progress_signal = float(self._homeostasis.state.get("intrinsic_reward", 0.0))
         extrinsic_signal = float(self._homeostasis.state.get("extrinsic_reward", 0.0))
         hedonic_signal = float(self._homeostasis.state.get("hedonic_reward", 0.0))
         fatigue = None
         if resource_monitor and hasattr(resource_monitor, "assess_fatigue"):
             try:
                 fatigue = float(resource_monitor.assess_fatigue(self._meta.metacognitive_history, self.arch))
             except Exception:
                 fatigue = None
         kernel_state = self._phenomenal_kernel.update(
             emotional_state=emotion_context,
             novelty=novelty,
             belief=belief,
             progress=progress_signal,
             extrinsic_reward=extrinsic_signal,
             hedonic_signal=hedonic_signal,
             fatigue=fatigue,
             alerts=system_alerts,
         )
         self.phenomenal_kernel_state.clear()
         self.phenomenal_kernel_state.update(kernel_state)
         self.phenomenal_kernel_state.setdefault("mode", self.current_mode)
         if self._last_system_snapshot:
             self.phenomenal_kernel_state["system_snapshot"] = dict(self._last_system_snapshot)
         setattr(self.arch, "phenomenal_kernel_state", self.phenomenal_kernel_state)
+        previous_mode = getattr(self, "current_mode", "travail")
         mode_info = self._mode_manager.update(self.phenomenal_kernel_state, urgent=urgent)
-        self.current_mode = mode_info.get("mode", self.current_mode)
+        self.current_mode = mode_info.get("mode", previous_mode)
         self.phenomenal_kernel_state["mode"] = self.current_mode
         self.phenomenal_kernel_state["flanerie_ratio"] = mode_info.get("flanerie_ratio")
         self.phenomenal_kernel_state["flanerie_budget_remaining"] = mode_info.get("flanerie_budget_remaining")
         self.phenomenal_kernel_state["urgent"] = urgent
+        if getattr(self, "phenomenal_journal", None) is not None:
+            try:
+                justification = None
+                interpretation = self.phenomenal_kernel_state.get("llm_interpretation")
+                if isinstance(interpretation, dict):
+                    justification = interpretation.get("justification")
+                self.phenomenal_journal.record_mode_transition(
+                    previous_mode=previous_mode,
+                    new_mode=self.current_mode,
+                    kernel_state={
+                        key: value
+                        for key, value in self.phenomenal_kernel_state.items()
+                        if isinstance(value, (int, float, str, bool, list, dict))
+                    },
+                    reason=justification,
+                )
+            except Exception:
+                pass
         slowdown = float(self.phenomenal_kernel_state.get("global_slowdown", 0.0) or 0.0)
         try:
             setattr(self.arch, "global_slowdown", slowdown)
         except Exception:
             pass
         try:
             setattr(self.arch, "current_mode", self.current_mode)
         except Exception:
             pass
         slowdown_factor = max(0.1, min(1.0, 1.0 - 0.7 * slowdown))
         for queue, base in self._job_base_budgets.items():
             factor = slowdown_factor * self._need_budget_factor(queue)
             if self.current_mode == "flanerie" and queue == "background":
                 factor *= 0.5
             target = max(1, math.ceil(base * factor))
             self.job_manager.budgets.setdefault(queue, {})["max_running"] = target
         hedonic_gain = float(self.phenomenal_kernel_state.get("hedonic_reward", 0.0))
         budget_remaining = float(
             self.phenomenal_kernel_state.get("flanerie_budget_remaining", 0.0) or 0.0
         )
         if (
             self.current_mode == "flanerie"
             and hedonic_gain >= self._hedonic_reward_min_gain
             and budget_remaining > 0.0
             and (time.time() - self._last_intrinsic_reward_ts) > self._intrinsic_reward_cooldown
         ):
             reward_engine = getattr(self.arch, "reward_engine", None)
             if reward_engine and hasattr(reward_engine, "register_intrinsic_reward"):
                 try:
                     reward_engine.register_intrinsic_reward(
                         "phenomenal_kernel",
                         hedonic_gain,
                         context={"phenomenal_kernel": dict(self.phenomenal_kernel_state), "mode": self.current_mode},
                     )
                     self._last_intrinsic_reward_ts = time.time()
                 except Exception:
                     pass
 
         contexts: List[Dict[str, Any]] = []
         for scored_trigger in selected:
             ctx = self._run_pipeline(scored_trigger.trigger)
             contexts.append(ctx)
 
         self.consolidate()
+        try:
+            self.phenomenal_recall.prime_for_digest(
+                self.memory.store,
+                kernel_state=self.phenomenal_kernel_state,
+                homeostasis=getattr(self._homeostasis, "state", {}),
+            )
+        except Exception:
+            pass
         r_intr, r_extr = self.emotion_homeostasis_cycle()
+        try:
+            self.phenomenal_journal.audit_against(
+                "homeostasis",
+                {
+                    "intrinsic_reward": float(r_intr),
+                    "extrinsic_reward": float(r_extr),
+                    "hedonic_reward": float(self._homeostasis.state.get("hedonic_reward", 0.0)),
+                },
+                tolerance=0.2,
+            )
+        except Exception:
+            pass
+        try:
+            self.phenomenal_questioner.maybe_question(self.phenomenal_kernel_state)
+        except Exception:
+            pass
         assessment, _ = self.meta_cycle()
         self.planning_cycle()
         self.action_cycle()
         self.proposals_cycle()
 
         selected_snapshot = []
         for item in selected:
             trigger = getattr(item, "trigger", None)
             trigger_type = getattr(trigger, "type", None)
             trigger_name = getattr(trigger_type, "name", str(trigger_type)) if trigger_type else None
             selected_snapshot.append(
                 {
                     "type": trigger_name,
                     "priority": getattr(item, "priority", None),
                 }
             )
 
         llm_payload = {
             "mode": self.current_mode,
             "urgent": urgent,
             "system_alerts": system_alerts,
             "assessment": assessment,
             "phenomenal_state": {
                 key: value
                 for key, value in self.phenomenal_kernel_state.items()
@@ -2988,50 +3065,77 @@ class Orchestrator:
                             pass
                     try:
                         self.self_model.register_decision(
                             {
                                 "decision_id": self._current_decision_id,
                                 "topic": ctx.get("topic") or "__generic__",
                                 "action": (decision.get("action", {}) or {}).get("type"),
                                 "expected": float((decision.get("expected") or {}).get("score", 1.0)),
                                 "obtained": float(obtained_score),
                                 "trace_id": self._current_trace_id,
                                 "ts": time.time(),
                             }
                         )
                     except Exception:
                         pass
                     try:
                         jm = getattr(self, "job_manager", None)
                         if jm and hasattr(jm, "snapshot_identity_view"):
                             view = jm.snapshot_identity_view() or {}
                             self.self_model.update_work(
                                 current=view.get("current"),
                                 recent=view.get("recent"),
                             )
                     except Exception:
                         pass
+                    try:
+                        decision_action = (ctx.get("decision", {}) or {}).get("action", {})
+                        if isinstance(decision_action, dict):
+                            action_label = (
+                                decision_action.get("desc")
+                                or decision_action.get("text")
+                                or decision_action.get("name")
+                                or decision_action.get("type")
+                                or "une action"
+                            )
+                        else:
+                            action_label = "une action"
+                        summary = f"J'exécute {action_label}" if action_label else "J'exécute une action"
+                        expected_score = (ctx.get("expected") or {}).get("score")
+                        try:
+                            expected_value = float(expected_score) if expected_score is not None else None
+                        except Exception:
+                            expected_value = None
+                        self._phenomenal_record_action(
+                            stage="ACT",
+                            ctx=ctx,
+                            summary=summary,
+                            expected=expected_value,
+                            obtained=float(obtained_score),
+                        )
+                    except Exception:
+                        pass
                 elif stg is Stage.FEEDBACK:
                     exp = float(ctx["expected"].get("score", 1.0))
                     obt = float((ctx.get("obtained") or {"score": 0.0}).get("score", 0.0))
                     err = abs(obt - exp)
                     ctx["scratch"]["raw_prediction_error"] = err
                     reward_signal = max(0.0, min(1.0, 1.0 - err))
                     success = obt >= exp
                     if success:
                         reward_signal = max(reward_signal, 0.95)
                     scratch = ctx.setdefault("scratch", {})
 
                     def _metric(key: str, default: float) -> float:
                         try:
                             return float(scratch.get(key, default))
                         except (TypeError, ValueError):
                             return float(default)
 
                     reward_features = {
                         "memory_consistency": _metric("memory_consistency", 0.5),
                         "transfer_success": _metric("transfer_success", 0.5),
                         "explanatory_adequacy": _metric("explanatory_adequacy", 0.5),
                         "social_appraisal": _metric("social_appraisal", 0.5),
                         "calibration_gap": _metric("calibration_gap", 0.3),
                         "clarification_penalty": 1.0
                         if (ctx.get("decision", {}).get("action", {}).get("type") == "clarify")
@@ -3061,50 +3165,63 @@ class Orchestrator:
                         ctx.get("scratch", {}).get("contradiction", False)
                     )
                     self.memory.store.add(
                         {
                             "kind": "feedback",
                             "pipe": pipe,
                             "mode": ctx["mode"].name if ctx.get("mode") else None,
                             "err": err,
                         }
                     )
                     mode_name = ctx["mode"].name if ctx.get("mode") else "unknown"
                     if policy_engine and hasattr(policy_engine, "update_outcome"):
                         try:
                             policy_engine.update_outcome(mode_name, ok=success)
                         except Exception:
                             pass
                     ctx.setdefault("scratch", {})
                     if "prediction_error" not in ctx["scratch"]:
                         ctx["scratch"]["prediction_error"] = 0.5  # valeur neutre
 
                     # Renforcement de l'habitude pour (action_type :: contexte)
                     try:
                         EvolutionManager.shared().reinforce(ctx)
                     except Exception:
                         pass
+                    try:
+                        feedback_summary = (
+                            "Le résultat confirme mon action" if success else "Le résultat contredit mon action"
+                        )
+                        self._phenomenal_record_action(
+                            stage="FEEDBACK",
+                            ctx=ctx,
+                            summary=f"{feedback_summary} (erreur={err:.2f}, récompense={reward_signal:.2f})",
+                            expected=exp,
+                            obtained=obt,
+                        )
+                    except Exception:
+                        pass
                 elif stg is Stage.LEARN:
                     self.cognition.evolution.reinforce(ctx)
                 elif stg is Stage.UPDATE:
                     consolidation = self.memory.consolidator.maybe_consolidate()
                     if isinstance(consolidation, dict):
                         new_items: List[Dict[str, Any]] = []
                         for lesson in consolidation.get("lessons", []) or []:
                             new_items.append({"kind": "lesson", "text": lesson})
                         for proposal in consolidation.get("proposals", []) or []:
                             if isinstance(proposal, dict):
                                 new_items.append({"kind": proposal.get("kind", "proposal"), "data": proposal})
                         for item in new_items:
                             try:
                                 self._sj_new_items_queue.append(item)
                             except Exception:
                                 break
                     prediction_error = float(ctx.get("scratch", {}).get("prediction_error", 0.0))
                     memory_consistency = float(ctx.get("scratch", {}).get("memory_consistency", 0.5))
                     transfer_success = float(ctx.get("scratch", {}).get("transfer_success", 0.5))
                     explanatory_adequacy = float(ctx.get("scratch", {}).get("explanatory_adequacy", 0.5))
                     social_appraisal = float(ctx.get("scratch", {}).get("social_appraisal", 0.5))
                     clarification_penalty = (
                         1.0
                         if (ctx.get("decision", {}).get("action", {}).get("type") == "clarify")
                         else 0.0
@@ -3173,50 +3290,55 @@ class Orchestrator:
                             reward_features=sj_features,
                             context=ema_context,
                         )
                     except Exception:
                         pass
 
                     if understanding_agg:
                         try:
                             U = understanding_agg.compute(
                                 topic=current_topic,
                                 prediction_error=prediction_error,
                                 memory_consistency=memory_consistency,
                                 transfer_success=transfer_success,
                                 explanatory_adequacy=explanatory_adequacy,
                                 social_appraisal=social_appraisal,
                                 clarification_penalty=clarification_penalty,
                                 calibration_gap=calibration_gap,
                             )
                         except Exception:
                             U = SimpleNamespace(U_topic=0.5, U_global=0.5)
                     else:
                         U = SimpleNamespace(U_topic=0.5, U_global=0.5)
 
                     scratch = ctx.setdefault("scratch", {})
                     scratch["clarification_penalty"] = clarification_penalty
+                    scratch["calibration_gap"] = calibration_gap
+                    scratch["memory_consistency"] = memory_consistency
+                    scratch["transfer_success"] = transfer_success
+                    scratch["explanatory_adequacy"] = explanatory_adequacy
+                    scratch["social_appraisal"] = social_appraisal
                     scratch.setdefault("understanding", {})
                     scratch["understanding"].update(
                         {
                             "topic": current_topic,
                             "U_topic": float(getattr(U, "U_topic", 0.5)),
                             "U_global": float(getattr(U, "U_global", 0.5)),
                         }
                     )
 
                     snap = (
                         monitor.snapshot()
                         if monitor
                         else SimpleNamespace(thinking_score=0.5, depth=0)
                     )
 
                     if hasattr(self.memory, "store") and hasattr(self.memory.store, "add"):
                         self.memory.store.add(
                             {
                                 "kind": "self_judgment",
                                 "topic": current_topic,
                                 "scores": {
                                     "U_topic": U.U_topic,
                                     "U_global": U.U_global,
                                     "thinking": getattr(snap, "thinking_score", 0.5),
                                     "calibration_gap": calibration_gap,
@@ -3435,52 +3557,311 @@ class Orchestrator:
                                 {"goal_kind": "ClarifyUserIntent", "topic": current_topic},
                             )
                         )
 
                     self_trust = (
                         getattr(getattr(selfhood, "traits", SimpleNamespace()), "self_trust", 1.0)
                         if selfhood
                         else 1.0
                     )
                     if (
                         self_trust < 0.45
                         and policy_engine
                         and hasattr(policy_engine, "set_uncertainty_disclosure")
                     ):
                         try:
                             policy_engine.set_uncertainty_disclosure(True)
                         except Exception:
                             pass
 
                     for t in followups:
                         try:
                             self._pending_triggers.append(t)
                         except Exception:
                             pass
 
+                    try:
+                        understanding_stats = ctx.get("scratch", {}).get("understanding", {})
+                        u_topic = float(understanding_stats.get("U_topic", getattr(U, "U_topic", 0.5)))
+                        u_global = float(understanding_stats.get("U_global", getattr(U, "U_global", 0.5)))
+                        summary = (
+                            f"Je me réévalue : compréhension globale={u_global:.2f},"
+                            f" précision locale={u_topic:.2f}, écart de calibration={calibration_gap:.2f}"
+                        )
+                        self._phenomenal_record_action(
+                            stage="UPDATE",
+                            ctx=ctx,
+                            summary=summary,
+                            expected=None,
+                            obtained=None,
+                        )
+                        if getattr(self, "phenomenal_journal", None) is not None:
+                            try:
+                                self.phenomenal_journal.audit_against(
+                                    "understanding",
+                                    {
+                                        "U_topic": u_topic,
+                                        "U_global": u_global,
+                                        "calibration_gap": float(calibration_gap),
+                                    },
+                                    tolerance=0.1,
+                                )
+                            except Exception:
+                                pass
+                    except Exception:
+                        pass
+
             finally:
                 mem_after = _get_process_memory_kb()
                 duration_ms = 1000.0 * (time.time() - stage_start)
                 delta_mem_mb = max(0.0, (mem_after - mem_before) / 1024.0)
                 stage_metrics.append({
                     "stage": stg.name,
                     "duration_ms": duration_ms,
                     "delta_mem_mb": round(delta_mem_mb, 4),
                 })
         pipeline_duration_ms = 1000.0 * (time.time() - pipeline_start)
         ctx["scratch"]["pipeline"]["duration_ms"] = pipeline_duration_ms
         if telemetry:
             try:
                 telemetry.log(
                     "pipeline_run",
                     "cognition",
                     {
                         "pipeline": pipe,
                         "family": selection.family,
                         "reason": selection.reason,
                         "duration_ms": pipeline_duration_ms,
                         "stages": stage_metrics,
                     },
                 )
             except Exception:
                 pass
         return ctx
+
+    def _phenomenal_identity_snapshot(self) -> Tuple[List[str], List[str]]:
+        values: List[str] = []
+        principles: List[str] = []
+        try:
+            persona = getattr(self.self_model, "persona", {})
+            if isinstance(persona, dict):
+                raw_values = persona.get("values")
+                if isinstance(raw_values, list):
+                    values.extend(str(val) for val in raw_values if isinstance(val, str))
+            identity = getattr(self.self_model, "identity", {})
+            if isinstance(identity, dict):
+                declared = identity.get("values")
+                if isinstance(declared, list):
+                    values.extend(str(val) for val in declared if isinstance(val, str))
+                principle_items = identity.get("principles")
+                if isinstance(principle_items, list):
+                    principles.extend(str(item) for item in principle_items if isinstance(item, str))
+                commitments = identity.get("commitments", {})
+                if isinstance(commitments, dict):
+                    by_key = commitments.get("by_key")
+                    if isinstance(by_key, dict):
+                        for key, info in by_key.items():
+                            if isinstance(info, dict) and info.get("active"):
+                                principles.append(str(key))
+        except Exception:
+            pass
+        if values:
+            values = sorted({val.strip() for val in values if val})
+        if principles:
+            principles = sorted({val.strip() for val in principles if val})
+        return values, principles
+
+    def _phenomenal_homeostasis_snapshot(self) -> Dict[str, Any]:
+        snapshot: Dict[str, Any] = {}
+        state = getattr(self._homeostasis, "state", {})
+        if isinstance(state, dict):
+            for key in ("intrinsic_reward", "extrinsic_reward", "hedonic_reward"):
+                value = state.get(key)
+                if isinstance(value, (int, float)):
+                    snapshot[key] = float(value)
+            drives = state.get("drives")
+            if isinstance(drives, Mapping):
+                ranked = sorted(
+                    (
+                        (str(name), float(val))
+                        for name, val in drives.items()
+                        if isinstance(val, (int, float))
+                    ),
+                    key=lambda item: item[1],
+                    reverse=True,
+                )
+                if ranked:
+                    snapshot["drives"] = {name: value for name, value in ranked[:5]}
+        return snapshot
+
+    def _phenomenal_emotion_snapshot(self) -> Dict[str, Any]:
+        emotions: Dict[str, Any] = {}
+        try:
+            emo_state = self.emotions.read()
+        except Exception:
+            emo_state = None
+        if emo_state is not None:
+            for key in ("valence", "arousal", "dominance"):
+                try:
+                    emotions[key] = float(getattr(emo_state, key))
+                except Exception:
+                    continue
+            label = getattr(emo_state, "label", None)
+            if not label:
+                label = getattr(emo_state, "state", None)
+            if label:
+                emotions["label"] = str(label)
+        kernel_state = getattr(self, "phenomenal_kernel_state", {})
+        if isinstance(kernel_state, dict):
+            interpretation = kernel_state.get("llm_interpretation")
+            if isinstance(interpretation, dict):
+                state_name = interpretation.get("current_state")
+                if state_name:
+                    emotions.setdefault("narrative", str(state_name))
+        return emotions
+
+    def _phenomenal_sensation_snapshot(self) -> Dict[str, Any]:
+        sensations: Dict[str, Any] = {}
+        kernel_state = getattr(self, "phenomenal_kernel_state", {})
+        if isinstance(kernel_state, dict):
+            for key in (
+                "energy",
+                "arousal",
+                "resonance",
+                "surprise",
+                "fatigue",
+                "hedonic_reward",
+                "global_slowdown",
+            ):
+                value = kernel_state.get(key)
+                if isinstance(value, (int, float)):
+                    sensations[key] = float(value)
+        return sensations
+
+    def _phenomenal_metric_snapshot(self, ctx: Mapping[str, Any]) -> Dict[str, float]:
+        metrics: Dict[str, float] = {}
+        if not isinstance(ctx, Mapping):
+            return metrics
+        scratch = ctx.get("scratch")
+        if isinstance(scratch, Mapping):
+            for key in (
+                "priority",
+                "prediction_error",
+                "sj_reward",
+                "sj_success",
+                "memory_consistency",
+                "transfer_success",
+                "explanatory_adequacy",
+                "social_appraisal",
+                "clarification_penalty",
+                "calibration_gap",
+            ):
+                value = scratch.get(key)
+                if isinstance(value, (int, float)):
+                    metrics[key] = float(value)
+            understanding = scratch.get("understanding")
+            if isinstance(understanding, Mapping):
+                for key in ("U_topic", "U_global"):
+                    value = understanding.get(key)
+                    if isinstance(value, (int, float)):
+                        metrics[key] = float(value)
+        expected = ctx.get("expected")
+        if isinstance(expected, Mapping):
+            uncertainty = expected.get("uncertainty")
+            if isinstance(uncertainty, (int, float)):
+                metrics["uncertainty"] = float(uncertainty)
+        return metrics
+
+    def _phenomenal_action_context(
+        self,
+        ctx: Mapping[str, Any],
+        *,
+        metrics: Optional[Mapping[str, float]] = None,
+    ) -> Dict[str, Any]:
+        payload: Dict[str, Any] = {}
+        trigger = getattr(self, "_current_trigger", None)
+        if trigger is not None:
+            payload["trigger_type"] = getattr(getattr(trigger, "type", None), "name", None)
+            meta = getattr(trigger, "meta", None)
+            if isinstance(meta, Mapping):
+                payload["trigger_meta"] = {
+                    key: meta[key]
+                    for key in ("importance", "immediacy", "reversibility", "source")
+                    if key in meta
+                }
+        if isinstance(ctx, Mapping):
+            scratch = ctx.get("scratch")
+            if isinstance(scratch, Mapping):
+                priority = scratch.get("priority")
+                if isinstance(priority, (int, float)):
+                    payload["priority"] = float(priority)
+                vetoes = scratch.get("policy_vetoes")
+                if isinstance(vetoes, (list, tuple)):
+                    payload["policy_vetoes"] = [str(v) for v in vetoes[:4]]
+                habit_payload = scratch.get("habit_payload")
+                if isinstance(habit_payload, Mapping):
+                    payload["habit"] = {
+                        "name": habit_payload.get("name")
+                        or (habit_payload.get("habit") or {}).get("name"),
+                        "confidence": habit_payload.get("confidence"),
+                    }
+            decision = ctx.get("decision")
+            if isinstance(decision, Mapping):
+                action = decision.get("action")
+                if isinstance(action, Mapping):
+                    payload["action"] = {
+                        "type": action.get("type"),
+                        "desc": action.get("desc") or action.get("text"),
+                    }
+                payload["reason"] = decision.get("reason") or decision.get("rationale")
+            gaps = ctx.get("gaps")
+            if isinstance(gaps, list):
+                payload["gaps"] = [str(g) for g in gaps[:5]]
+        if metrics:
+            payload["metrics"] = {key: float(value) for key, value in metrics.items()}
+        return {k: v for k, v in payload.items() if v is not None}
+
+    def _phenomenal_record_action(
+        self,
+        *,
+        stage: str,
+        ctx: Mapping[str, Any],
+        summary: str,
+        expected: Optional[float],
+        obtained: Optional[float],
+    ) -> None:
+        journal = getattr(self, "phenomenal_journal", None)
+        if journal is None:
+            return
+        try:
+            mode_value = ctx.get("mode") if isinstance(ctx, Mapping) else None
+            if isinstance(mode_value, ActMode):
+                mode = mode_value.name.lower()
+            else:
+                mode = str(mode_value) if mode_value else None
+            topic = ctx.get("topic") if isinstance(ctx, Mapping) else None
+            values, principles = self._phenomenal_identity_snapshot()
+            homeostasis = self._phenomenal_homeostasis_snapshot()
+            emotions = self._phenomenal_emotion_snapshot()
+            sensations = self._phenomenal_sensation_snapshot()
+            metrics = self._phenomenal_metric_snapshot(ctx)
+            context_payload = self._phenomenal_action_context(ctx, metrics=metrics)
+            entry = journal.record_action(
+                stage=stage,
+                mode=mode,
+                topic=str(topic) if topic else None,
+                summary=summary,
+                expected=expected,
+                obtained=obtained,
+                values=values,
+                principles=principles,
+                homeostasis=homeostasis,
+                emotions=emotions,
+                sensations=sensations,
+                context=context_payload,
+            )
+            try:
+                ctx.setdefault("scratch", {})["phenomenal_episode"] = entry
+            except Exception:
+                pass
+        except Exception:
+            pass
